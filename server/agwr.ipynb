{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d55a0412-63ed-4871-b473-4c965db23a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format='retina'\n",
    "import json\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f63f763-cfe8-4394-b8f5-e7d1730e773d",
   "metadata": {},
   "source": [
    "## KC sample creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b023eb55-a7ba-4e7c-9774-b8cb242d81e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load data and drop duplicate locations\n",
    "df=pd.read_csv('kc_house_data.csv')\n",
    "df.drop_duplicates(subset=['lat','long'],inplace=True)\n",
    "\n",
    "# 2. restrict data\n",
    "def inBounds(lat,long):\n",
    "    return lat<47.65 and lat>47.59 and long>-122.33 and long<-122.28 and (lat > -0.925*long - 65.547)\n",
    "df=df[df.apply(lambda x: inBounds(x['lat'],x['long']),axis=1)]\n",
    "fig,ax=plt.subplots()\n",
    "ax.set_title(f'n={len(df)}')\n",
    "ax.scatter(df['long'],df['lat'])\n",
    "\n",
    "# 3. save data\n",
    "df.to_csv('kc_house_sample.csv')\n",
    "print(f\"center={df['longitude'].mean(),df['latitude'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a330b8d",
   "metadata": {},
   "source": [
    "## NY sample creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8efe1b5-53db-42bd-a3a1-015fd80c385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load data and drop duplicate locations\n",
    "df=pd.read_csv('AB_NYC_2019.csv')\n",
    "df.drop_duplicates(subset=['latitude','longitude'],inplace=True)\n",
    "\n",
    "# 2. restrict data\n",
    "df=df[df.apply(lambda x: x['neighbourhood_group']=='Bronx',axis=1)]\n",
    "fig,ax=plt.subplots()\n",
    "ax.set_title(f'n={len(df)}')\n",
    "ax.scatter(df['longitude'],df['latitude'])\n",
    "\n",
    "# 3. replace NaN with 0 in reviews_per_month\n",
    "df['reviews_per_month'].fillna(0,inplace=True)\n",
    "\n",
    "# 3. save data\n",
    "df.to_csv('ny_airbnb_sample.csv')\n",
    "print(f\"center={df['longitude'].mean(),df['latitude'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280e29a7",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fe23f33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to \"/users/nicolaslee/agwr/trained_models/mgwr_random_forest_kc.ts\"\n",
      "Data written to \"/users/nicolaslee/agwr/trained_models/mgwr_random_forest_ny.ts\"\n",
      "Data written to \"/users/nicolaslee/agwr/trained_models/mgwr_neural_network_kc.ts\"\n",
      "Data written to \"/users/nicolaslee/agwr/trained_models/mgwr_neural_network_ny.ts\"\n",
      "Data written to \"/users/nicolaslee/agwr/trained_models/mgwr_xgb_kc.ts\"\n",
      "Data written to \"/users/nicolaslee/agwr/trained_models/mgwr_xgb_ny.ts\"\n",
      "Data written to \"/users/nicolaslee/agwr/trained_models/smgwr_random_forest_kc.ts\"\n",
      "Data written to \"/users/nicolaslee/agwr/trained_models/smgwr_random_forest_ny.ts\"\n",
      "Data written to \"/users/nicolaslee/agwr/trained_models/smgwr_neural_network_kc.ts\"\n",
      "Data written to \"/users/nicolaslee/agwr/trained_models/smgwr_neural_network_ny.ts\"\n",
      "Data written to \"/users/nicolaslee/agwr/trained_models/smgwr_xgb_kc.ts\"\n",
      "Data written to \"/users/nicolaslee/agwr/trained_models/smgwr_xgb_ny.ts\"\n"
     ]
    }
   ],
   "source": [
    "# 1. set names for coefficient files\n",
    "# 1.1 get names in the order they are created\n",
    "modules_all_spatial = [\"mgwr\",\"smgwr\"]\n",
    "modules_all_ml = [\"random_forest\",\"neural_network\",\"xgb\"]\n",
    "modules_all_data = [\"kc\", \"ny\"]\n",
    "names = list(\n",
    "    map(\n",
    "        lambda x: \"_\".join(x),\n",
    "        list(itertools.product(modules_all_spatial, modules_all_ml, modules_all_data)),\n",
    "    )\n",
    ")\n",
    "# 1.2 get files in the order they are created\n",
    "directory = '/users/nicolaslee/agwr/trained_models'\n",
    "files = [f for f in os.listdir(directory) if f.startswith('coefficients')]\n",
    "files.sort()\n",
    "\n",
    "# 1.3 rename each file\n",
    "for name, file in zip(names, files):\n",
    "    old_file = os.path.join(directory, file)\n",
    "    new_file = os.path.join(directory, name + \"_coefficients.pkl\")\n",
    "    os.rename(old_file, new_file)\n",
    "\n",
    "# 2. generate data for each name\n",
    "# 2.1 load datasets\n",
    "kc = pd.read_csv('/users/nicolaslee/desktop/pyneapple-demo/server//kc_house_sample.csv')\n",
    "ny = pd.read_csv(\"/users/nicolaslee/desktop/pyneapple-demo/server/ny_airbnb_sample.csv\")\n",
    "for name in names:\n",
    "    is_kc = name[-2:] == \"kc\"\n",
    "    # 2.2 load coefficients\n",
    "    filename = os.path.join(directory, name + \"_coefficients.pkl\")\n",
    "    with open(filename, \"rb\") as file:\n",
    "        coefficients = pickle.load(file)\n",
    "    coefficient_min = coefficients.min(axis=0)\n",
    "    coefficient_median = np.median(coefficients, axis=0)\n",
    "    coefficient_max = coefficients.max(axis=0)\n",
    "    # 2.3 load predictions\n",
    "    predictions = pd.read_csv(os.path.join(directory,name+'_predictions.csv'))\n",
    "    if (is_kc):\n",
    "        df = kc.copy()\n",
    "    else:\n",
    "        df = ny.copy()\n",
    "    df['predicted']=predictions['predicted'].values\n",
    "    # 2.4 append parameters to end of file\n",
    "    filename = os.path.join(directory, name + \"_parameters.txt\")\n",
    "    with open(filename, \"a\") as file:\n",
    "        file.write(\n",
    "            \"coefficientMins:\"\n",
    "            + json.dumps(list(map(lambda x: json.loads(json.dumps(x)), coefficient_min)))\n",
    "            + \",coefficientMeds:\"\n",
    "            + json.dumps(list(map(lambda x: json.loads(json.dumps(x)), coefficient_median)))\n",
    "            + \",coefficientMaxes:\"\n",
    "            + json.dumps(list(map(lambda x: json.loads(json.dumps(x)), coefficient_max)))\n",
    "            + \",\"\n",
    "        )\n",
    "        print(f'Coefficient statistics written to \"{filename}\"')\n",
    "    # 2.5. write data in JSON\n",
    "    filename = os.path.join(directory, name + \".ts\")\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write('import { Dataset } from \"data/data\";')\n",
    "        file.write(\"const DATASET:Dataset = {\")\n",
    "        file.write(f'name:\"{name}\",')\n",
    "        file.write(\n",
    "            f'center:{[47.61729305740989, -122.30174365821095] if is_kc else [40.844, -73.87]},'\n",
    "        )\n",
    "        file.write(f'zoom:{14 if is_kc else 13},')\n",
    "        with open(os.path.join(directory, name + \"_parameters.txt\"),'r') as parameters_file:\n",
    "            file.write(parameters_file.read())\n",
    "        file.write(\"data: [\")\n",
    "\n",
    "        feature_labels = ([\n",
    "                                    \"intercept\",\n",
    "                                    \"bedrooms\",\n",
    "                                    \"bathrooms\",\n",
    "                                    \"sqft_living\",\n",
    "                                    \"sqft_lot\",\n",
    "                                    \"floors\",\n",
    "                                ]\n",
    "                                if is_kc\n",
    "                                else [\n",
    "                                    \"intercept\",\n",
    "                                    \"minimum_nights\",\n",
    "                                    \"number_of_reviews\",\n",
    "                                    \"reviews_per_month\",\n",
    "                                    \"host_listings_count\",\n",
    "                                    \"availability_365\",\n",
    "                                ])\n",
    "\n",
    "        def toJSON(df):\n",
    "            i = 0\n",
    "            for _, row in df.iterrows():\n",
    "\n",
    "                coefficient_dict = {\n",
    "                    feature: value\n",
    "                    for feature, value in zip(\n",
    "                        feature_labels,\n",
    "                        list(map(lambda x: json.loads(json.dumps(x)), coefficients[i])),\n",
    "                    )\n",
    "                }\n",
    "\n",
    "                file.write(\n",
    "                    \"{latitude:\"\n",
    "                    + str(row[\"latitude\"])\n",
    "                    + \", longitude:\"\n",
    "                    + str(row[\"longitude\"])\n",
    "                    + \",actual:\"\n",
    "                    + str(row[\"price\"])\n",
    "                    + \",predicted:\"\n",
    "                    + str(int(row[\"predicted\"]))\n",
    "                    + \",coefficients:\"\n",
    "                    + json.dumps(coefficient_dict,indent=4)\n",
    "                    + \"},\"\n",
    "                )\n",
    "                i += 1\n",
    "\n",
    "        toJSON(df)\n",
    "        file.write(\"]};export default DATASET;\")\n",
    "        print(f'Data written to \"{filename}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
